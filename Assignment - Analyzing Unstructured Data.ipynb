{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check NAs\n",
    "- check unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. Do text preprocessing (e.g., stopword removal, lemmatization, stemming, etc.)\n",
    "2. TF-IDF text representation\n",
    "3. Run LDA\n",
    "4. Identify the optimal number of topics\n",
    "5. Show top 10 words for each topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The dataset consists 2885 datasets information in 15 columns:\n",
    "\n",
    "- Title\n",
    "- Subtitle\n",
    "- Owner\n",
    "- Vote\n",
    "- Last update\n",
    "- Tags\n",
    "- Datatype\n",
    "- Size\n",
    "- License\n",
    "- Views\n",
    "- Downloads\n",
    "- Kernels\n",
    "- Topics\n",
    "- URL\n",
    "- Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that the variables title, subtitle as well as description can be used for topic modeling. Therefore we start with title, go on with subtitle and finish with description to get the best possible topics out of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title,Subtitle,Owner,Votes,Last Update,Tags,Data Type,Size,License,Views,Download,Kernels,Topics,URL,Description\\n',\n",
       " 'Credit Card Fraud Detection,Anonymized credit card transactions labeled as fraudulent or genuine,Machine Learning Group - ULB,1233,2016-11-05,\"crime\\n',\n",
       " 'finance\",CSV,144 MB,ODbL,\"440,221 views\",\"52,793 downloads\",\"1,778 kernels\",26 topics,https://www.kaggle.com/mlg-ulb/creditcardfraud,\"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\\n',\n",
       " \"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\\n\",\n",
       " 'Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\\n',\n",
       " 'The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\\n',\n",
       " 'Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\"\\n',\n",
       " 'European Soccer Database,\"25k+ matches, players & teams attributes for European Professional Football\",Hugo Mathien,1035,2016-10-24,\"association football\\n',\n",
       " 'europe\",SQLite,299 MB,ODbL,\"393,924 views\",\"46,025 downloads\",\"1,459 kernels\",75 topics,https://www.kaggle.com/hugomathien/soccer,\"The ultimate Soccer database for data analysis and machine learning\\n',\n",
       " 'What you get:\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dataset.csv','r') as fh:\n",
    "    lines = fh.readlines()\n",
    "fh.close()\n",
    "lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Subtitle</th>\n",
       "      <th>Owner</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Last Update</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>License</th>\n",
       "      <th>Views</th>\n",
       "      <th>Download</th>\n",
       "      <th>Kernels</th>\n",
       "      <th>Topics</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Credit Card Fraud Detection</td>\n",
       "      <td>Anonymized credit card transactions labeled as...</td>\n",
       "      <td>Machine Learning Group - ULB</td>\n",
       "      <td>1233</td>\n",
       "      <td>2016-11-05</td>\n",
       "      <td>crime\\nfinance</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>440,221 views</td>\n",
       "      <td>52,793 downloads</td>\n",
       "      <td>1,778 kernels</td>\n",
       "      <td>26 topics</td>\n",
       "      <td>https://www.kaggle.com/mlg-ulb/creditcardfraud</td>\n",
       "      <td>The datasets contains transactions made by cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>European Soccer Database</td>\n",
       "      <td>25k+ matches, players &amp; teams attributes for E...</td>\n",
       "      <td>Hugo Mathien</td>\n",
       "      <td>1035</td>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>association football\\neurope</td>\n",
       "      <td>SQLite</td>\n",
       "      <td>299 MB</td>\n",
       "      <td>ODbL</td>\n",
       "      <td>393,924 views</td>\n",
       "      <td>46,025 downloads</td>\n",
       "      <td>1,459 kernels</td>\n",
       "      <td>75 topics</td>\n",
       "      <td>https://www.kaggle.com/hugomathien/soccer</td>\n",
       "      <td>The ultimate Soccer database for data analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMDB 5000 Movie Dataset</td>\n",
       "      <td>Metadata on ~5,000 movies from TMDb</td>\n",
       "      <td>The Movie Database (TMDb)</td>\n",
       "      <td>1018</td>\n",
       "      <td>2017-09-28</td>\n",
       "      <td>film</td>\n",
       "      <td>CSV</td>\n",
       "      <td>44 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>444,535 views</td>\n",
       "      <td>61,705 downloads</td>\n",
       "      <td>1,394 kernels</td>\n",
       "      <td>46 topics</td>\n",
       "      <td>https://www.kaggle.com/tmdb/tmdb-movie-metadata</td>\n",
       "      <td>Background\\nWhat can we say about the success ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Human Resources Analytics</td>\n",
       "      <td>Why are our best and most experienced employee...</td>\n",
       "      <td>Ludovic Benistant</td>\n",
       "      <td>832</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>employment</td>\n",
       "      <td>CSV</td>\n",
       "      <td>553 KB</td>\n",
       "      <td>CC4</td>\n",
       "      <td>309,644 views</td>\n",
       "      <td>47,350 downloads</td>\n",
       "      <td>1,772 kernels</td>\n",
       "      <td>32 topics</td>\n",
       "      <td>https://www.kaggle.com/ludobenistant/hr-analytics</td>\n",
       "      <td>This dataset is simulated\\nWhy are our best an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Global Terrorism Database</td>\n",
       "      <td>More than 170,000 terrorist attacks worldwide,...</td>\n",
       "      <td>START Consortium</td>\n",
       "      <td>785</td>\n",
       "      <td>2017-07-19</td>\n",
       "      <td>crime\\nterrorism\\ninternational relations</td>\n",
       "      <td>CSV</td>\n",
       "      <td>144 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>186,621 views</td>\n",
       "      <td>26,091 downloads</td>\n",
       "      <td>609 kernels</td>\n",
       "      <td>11 topics</td>\n",
       "      <td>https://www.kaggle.com/START-UMD/gtd</td>\n",
       "      <td>Context\\nInformation on more than 170,000 Terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>House Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Varun Kashyap.K.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "      <td>2 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>67 views</td>\n",
       "      <td>12 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 topics</td>\n",
       "      <td>https://www.kaggle.com/varunkashyapks/house-data</td>\n",
       "      <td>This dataset does not have a description yet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>Super Market Product</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Varun Kashyap.K.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>510 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>141 views</td>\n",
       "      <td>35 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 topics</td>\n",
       "      <td>https://www.kaggle.com/varunkashyapks/super-ma...</td>\n",
       "      <td>This dataset does not have a description yet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>Super Market Products</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Varun Kashyap.K.S.</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "      <td>179 MB</td>\n",
       "      <td>Other</td>\n",
       "      <td>138 views</td>\n",
       "      <td>8 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 topics</td>\n",
       "      <td>https://www.kaggle.com/varunkashyapks/super-ma...</td>\n",
       "      <td>This dataset does not have a description yet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>Sales Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kamau John</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "      <td>988 B</td>\n",
       "      <td>CC0</td>\n",
       "      <td>372 views</td>\n",
       "      <td>38 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 topics</td>\n",
       "      <td>https://www.kaggle.com/sophicist/sales-data</td>\n",
       "      <td>This dataset does not have a description yet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>Titanic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marcel</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CSV</td>\n",
       "      <td>88 KB</td>\n",
       "      <td>Other</td>\n",
       "      <td>337 views</td>\n",
       "      <td>57 downloads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0 topics</td>\n",
       "      <td>https://www.kaggle.com/mkempers/titanic</td>\n",
       "      <td>This dataset does not have a description yet.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2885 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title  \\\n",
       "0     Credit Card Fraud Detection   \n",
       "1        European Soccer Database   \n",
       "2         TMDB 5000 Movie Dataset   \n",
       "3       Human Resources Analytics   \n",
       "4       Global Terrorism Database   \n",
       "...                           ...   \n",
       "2880                   House Data   \n",
       "2881         Super Market Product   \n",
       "2882        Super Market Products   \n",
       "2883                   Sales Data   \n",
       "2884                      Titanic   \n",
       "\n",
       "                                               Subtitle  \\\n",
       "0     Anonymized credit card transactions labeled as...   \n",
       "1     25k+ matches, players & teams attributes for E...   \n",
       "2                   Metadata on ~5,000 movies from TMDb   \n",
       "3     Why are our best and most experienced employee...   \n",
       "4     More than 170,000 terrorist attacks worldwide,...   \n",
       "...                                                 ...   \n",
       "2880                                                NaN   \n",
       "2881                                                NaN   \n",
       "2882                                                NaN   \n",
       "2883                                                NaN   \n",
       "2884                                                NaN   \n",
       "\n",
       "                             Owner  Votes Last Update  \\\n",
       "0     Machine Learning Group - ULB   1233  2016-11-05   \n",
       "1                     Hugo Mathien   1035  2016-10-24   \n",
       "2        The Movie Database (TMDb)   1018  2017-09-28   \n",
       "3                Ludovic Benistant    832  2016-11-30   \n",
       "4                 START Consortium    785  2017-07-19   \n",
       "...                            ...    ...         ...   \n",
       "2880            Varun Kashyap.K.S.      1  2017-11-02   \n",
       "2881            Varun Kashyap.K.S.      1  2017-11-03   \n",
       "2882            Varun Kashyap.K.S.      1  2017-11-03   \n",
       "2883                    Kamau John      1  2017-11-02   \n",
       "2884                        Marcel      1  2017-11-03   \n",
       "\n",
       "                                           Tags Data Type    Size License  \\\n",
       "0                                crime\\nfinance       CSV  144 MB    ODbL   \n",
       "1                  association football\\neurope    SQLite  299 MB    ODbL   \n",
       "2                                          film       CSV   44 MB   Other   \n",
       "3                                    employment       CSV  553 KB     CC4   \n",
       "4     crime\\nterrorism\\ninternational relations       CSV  144 MB   Other   \n",
       "...                                         ...       ...     ...     ...   \n",
       "2880                                        NaN       CSV    2 MB   Other   \n",
       "2881                                        NaN     Other  510 MB   Other   \n",
       "2882                                        NaN       CSV  179 MB   Other   \n",
       "2883                                        NaN       CSV   988 B     CC0   \n",
       "2884                                        NaN       CSV   88 KB   Other   \n",
       "\n",
       "              Views          Download        Kernels     Topics  \\\n",
       "0     440,221 views  52,793 downloads  1,778 kernels  26 topics   \n",
       "1     393,924 views  46,025 downloads  1,459 kernels  75 topics   \n",
       "2     444,535 views  61,705 downloads  1,394 kernels  46 topics   \n",
       "3     309,644 views  47,350 downloads  1,772 kernels  32 topics   \n",
       "4     186,621 views  26,091 downloads    609 kernels  11 topics   \n",
       "...             ...               ...            ...        ...   \n",
       "2880       67 views      12 downloads            NaN   0 topics   \n",
       "2881      141 views      35 downloads            NaN   0 topics   \n",
       "2882      138 views       8 downloads            NaN   0 topics   \n",
       "2883      372 views      38 downloads            NaN   0 topics   \n",
       "2884      337 views      57 downloads            NaN   0 topics   \n",
       "\n",
       "                                                    URL  \\\n",
       "0        https://www.kaggle.com/mlg-ulb/creditcardfraud   \n",
       "1             https://www.kaggle.com/hugomathien/soccer   \n",
       "2       https://www.kaggle.com/tmdb/tmdb-movie-metadata   \n",
       "3     https://www.kaggle.com/ludobenistant/hr-analytics   \n",
       "4                  https://www.kaggle.com/START-UMD/gtd   \n",
       "...                                                 ...   \n",
       "2880   https://www.kaggle.com/varunkashyapks/house-data   \n",
       "2881  https://www.kaggle.com/varunkashyapks/super-ma...   \n",
       "2882  https://www.kaggle.com/varunkashyapks/super-ma...   \n",
       "2883        https://www.kaggle.com/sophicist/sales-data   \n",
       "2884            https://www.kaggle.com/mkempers/titanic   \n",
       "\n",
       "                                            Description  \n",
       "0     The datasets contains transactions made by cre...  \n",
       "1     The ultimate Soccer database for data analysis...  \n",
       "2     Background\\nWhat can we say about the success ...  \n",
       "3     This dataset is simulated\\nWhy are our best an...  \n",
       "4     Context\\nInformation on more than 170,000 Terr...  \n",
       "...                                                 ...  \n",
       "2880      This dataset does not have a description yet.  \n",
       "2881      This dataset does not have a description yet.  \n",
       "2882      This dataset does not have a description yet.  \n",
       "2883      This dataset does not have a description yet.  \n",
       "2884      This dataset does not have a description yet.  \n",
       "\n",
       "[2885 rows x 15 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Pandas libraries with alias 'pd' \n",
    "import pandas as pd \n",
    "pd.options.display.max_rows = 10\n",
    "d = pd.read_csv(\"dataset.csv\") \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2885 entries, 0 to 2884\n",
      "Data columns (total 15 columns):\n",
      "title          2885 non-null object\n",
      "subtitle       2413 non-null object\n",
      "owner          2885 non-null object\n",
      "votes          2885 non-null int64\n",
      "last update    2885 non-null object\n",
      "tags           1700 non-null object\n",
      "data type      2885 non-null object\n",
      "size           2885 non-null object\n",
      "license        2885 non-null object\n",
      "views          2874 non-null object\n",
      "download       2803 non-null object\n",
      "kernels        1276 non-null object\n",
      "topics         2243 non-null object\n",
      "url            2885 non-null object\n",
      "description    2874 non-null object\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 338.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# d.shape\n",
    "d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Credit Card Fraud Detection\n",
       "1          European Soccer Database\n",
       "2           TMDB 5000 Movie Dataset\n",
       "3         Human Resources Analytics\n",
       "4         Global Terrorism Database\n",
       "                   ...             \n",
       "2880                     House Data\n",
       "2881           Super Market Product\n",
       "2882          Super Market Products\n",
       "2883                     Sales Data\n",
       "2884                        Titanic\n",
       "Name: title, Length: 2885, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns = d.columns.str.lower()\n",
    "title = d['title']\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Anonymized credit card transactions labeled as...\n",
       "1       25k+ matches, players & teams attributes for E...\n",
       "2                     Metadata on ~5,000 movies from TMDb\n",
       "3       Why are our best and most experienced employee...\n",
       "4       More than 170,000 terrorist attacks worldwide,...\n",
       "                              ...                        \n",
       "2880                                                  NaN\n",
       "2881                                                  NaN\n",
       "2882                                                  NaN\n",
       "2883                                                  NaN\n",
       "2884                                                  NaN\n",
       "Name: subtitle, Length: 2885, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtitle = d['subtitle']\n",
    "subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       The datasets contains transactions made by cre...\n",
       "1       The ultimate Soccer database for data analysis...\n",
       "2       Background\\nWhat can we say about the success ...\n",
       "3       This dataset is simulated\\nWhy are our best an...\n",
       "4       Context\\nInformation on more than 170,000 Terr...\n",
       "                              ...                        \n",
       "2880        This dataset does not have a description yet.\n",
       "2881        This dataset does not have a description yet.\n",
       "2882        This dataset does not have a description yet.\n",
       "2883        This dataset does not have a description yet.\n",
       "2884        This dataset does not have a description yet.\n",
       "Name: description, Length: 2885, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description = d['description']\n",
    "description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence tokenization and word tokenziation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert text into NLTK Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Credit'],\n",
       " ['Card'],\n",
       " ['Fraud'],\n",
       " ['Detection'],\n",
       " ['European'],\n",
       " ['Soccer'],\n",
       " ['Database'],\n",
       " ['TMDB'],\n",
       " ['5000'],\n",
       " ['Movie']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_words = word_tokenize(title[0])\n",
    "title_words = []\n",
    "title_sentences = []\n",
    "for word in title:\n",
    "    # title_words.append(word_tokenize(word))\n",
    "    sentence = word_tokenize(word)\n",
    "    title_sentences.append(word_tokenize(word))\n",
    "    for word2 in sentence:\n",
    "        title_words.append(word_tokenize(word2))\n",
    "title_words[:10]\n",
    "# subtitle_words = word_tokenize(subtitle)\n",
    "# description_words = word_tokenize(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11271"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(title_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: ['Credit', 'Card', 'Fraud', 'Detection'] ['European', 'Soccer', 'Database'] ['TMDB', '5000', 'Movie', 'Dataset'] ['Human', 'Resources', 'Analytics'] ['Global', 'Terrorism', 'Database'] ['Bitcoin', 'Historical', 'Data'] ['Kaggle', 'ML', 'and', 'Data', 'Science', 'Survey', ',', '2017'] ['Iris', 'Species']...>\n"
     ]
    }
   ],
   "source": [
    "# title_text = nltk.Text(title_words)\n",
    "# print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(stop_words='english',) # we create a customized vectorizer, remove english stop words\n",
    "X = vectorizer2.fit_transform(title_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-b6bb31680254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we create a customized vectorizer, remove english stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', ) # we create a customized vectorizer, remove english stop words\n",
    "X = vectorizer.fit_transform(title_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 8] nodename\n",
      "[nltk_data]     nor servname provided, or not known>\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11271"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops_en = stopwords.words('english')\n",
    "print(stops_en[:10])\n",
    "# to do: remove stop words\n",
    "len(title_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11271"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use portar stemming method\n",
    "portar = nltk.PorterStemmer() # is best practise\n",
    "portar.stem(str(title_words[0]))\n",
    "title_words_clean = []\n",
    "for word in title_words:\n",
    "    title_words_clean.append((portar.stem(str(word))))\n",
    "\n",
    "len(title_words_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit one python Jupyter notebook file with the filename following: YourFirstName_YourLastName.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
