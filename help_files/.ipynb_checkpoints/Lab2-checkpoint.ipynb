{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# to convert every document in binary vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['This is the first document, this document is about movies.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?',\n",
    "         ]\n",
    "\n",
    "vectorizer1 = CountVectorizer() # we create a default vectorizer\n",
    "# possible parameters of CountVectorizer(): lowercase, stopwords, tokenizer, ...\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "X = vectorizer1.fit_transform(docs) # transform 4 sentences in vector representations\n",
    "print(X)\n",
    "\n",
    "print('features/variables:\\n',vectorizer1.get_feature_names(),'\\n') # \\n is the line breaker\n",
    "\n",
    "print(X.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature names <br>\n",
    "count of word per sentence, e. g. document is twice in sentence one and two, zero in sentence three and once in document four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(stop_words='english',) # we create a customized vectorizer, remove english stop words\n",
    "X = vectorizer2.fit_transform(docs)\n",
    "\n",
    "print('features/variables:\\n',vectorizer2.get_feature_names(),'\\n') # \\n is the line breaker\n",
    "\n",
    "print(X.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer3 = CountVectorizer(binary=True,stop_words='english',ngram_range=(1,2)) # we create a customized vectorizer\n",
    "X = vectorizer3.fit_transform(docs)\n",
    "\n",
    "print('features/variables:\\n',vectorizer3.get_feature_names(),'\\n') # \\n is the line breaker\n",
    "\n",
    "print(X.toarray()) # see vector representing uni- and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorizer\n",
    "**Gold standard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer4 = TfidfVectorizer(binary=True,stop_words='english',ngram_range=(1,1)) # we create a customized vectorizer\n",
    "\n",
    "X = vectorizer4.fit_transform(docs)\n",
    "\n",
    "print('features/variables:\\n',vectorizer4.get_feature_names(),'\\n') # \\n is the line breaker\n",
    "\n",
    "print(X.toarray())\n",
    "# return tf idf values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of document rows\n",
    "fh = open('spambase.txt', 'r')\n",
    "lines = fh.readlines()\n",
    "fh.close()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [3, 5, 7, 8, 7, 5]\n",
    "print(lst[0:4])\n",
    "print(lst[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'I like, st. gallen'\n",
    "sent.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "with open('spambase.txt','r') as fh:\n",
    "    lines = fh.readlines()\n",
    "fh.close()\n",
    "\n",
    "X = [] # features\n",
    "Y = [] # target, label, class, category\n",
    "\n",
    "# iterate through lines\n",
    "for line in lines:\n",
    "    arr = line.strip().split(',') # split by delimiter ,\n",
    "    feature = arr[:-1] # features for that instance/line\n",
    "    trans_feature = [float(f) for f in feature] # convert each string into a numeric type\n",
    "    X.append(trans_feature)\n",
    "    \n",
    "    Y.append(arr[-1])\n",
    "\n",
    "# print(X[0]) # show the features for the first email\n",
    "# print(Y) # show labels for all emails\n",
    "\n",
    "gnb = GaussianNB() # build a NB model\n",
    "learning = gnb.fit(X, Y) # learn parameters: training process\n",
    "y_pred = learning.predict(X) # get estimated Y values for training dataset\n",
    "# print(y_pred[:10])\n",
    "print(y_pred[-10:])\n",
    "\n",
    "y_pred_prob = learning.predict_proba(X)\n",
    "# print(y_pred_prob[:10])\n",
    "print(y_pred_prob[-10:])\n",
    "# left column: probability that is a o, right column: probability that it is one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check list comprehension in normal form\n",
    "\n",
    "X = [] # features\n",
    "Y = [] # target, label, class, category\n",
    "\n",
    "# iterate through lines\n",
    "for line in lines:\n",
    "    arr = line.strip().split(',') # split by delimiter ,\n",
    "    feature = arr[:-1] # features for that instance/line\n",
    "    # trans_feature = \n",
    "    for f in feature:\n",
    "        float(f)\n",
    "        print(f)\n",
    "    # [float(f) for f in feature] # convert each string into a numeric type\n",
    "    # X.append(trans_feature)\n",
    "    \n",
    "    # Y.append(arr[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes - evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, random_state=22) # split the dataset into training and testing\n",
    "# there are a lot of parameters for train_test_split():\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "learning = gnb.fit(x_train,y_train)\n",
    "score = gnb.score(x_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_results = cross_val_score(gnb, X, Y,\n",
    "                             cv = 10,    #10 fold cross validation\n",
    "                             scoring = \"accuracy\",\n",
    "                             verbose = 1 # decide how detailed results will be shown. 0: only results, \n",
    "                                         # 1: with some running info (# runs with values, ...)\n",
    "                                         # 2: very detailed (every run, # runs with values, ...)\n",
    "                                         # verbose = langatmig\n",
    "                            )\n",
    "print('10 runs:',cv_results,'\\n')\n",
    "print('average:',cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=2) # create a random forest model\n",
    "\n",
    "clf.fit(X, Y)  # using training data to fit the model\n",
    "\n",
    "cv_results = cross_val_score(clf, X, Y, cv = 10, scoring='accuracy')\n",
    "\n",
    "print('average:',cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "reg = LogisticRegression(random_state=22)\n",
    "\n",
    "reg.fit(X, Y)  # using training data to fit the model\n",
    "\n",
    "cv_results = cross_val_score(reg, X, Y, cv = 10, scoring='accuracy', verbose= 0)\n",
    "\n",
    "print('average:',cv_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, f1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "Y_true = [int(y) for y in Y]\n",
    "\n",
    "clf.fit(X, Y_true) # re-fit the data\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "precision = precision_score(Y_true,y_pred)\n",
    "recall = recall_score(Y_true,y_pred)\n",
    "f1 = f1_score(Y_true,y_pred)\n",
    "\n",
    "print('precision:',precision,'\\n','recall:',recall,'\\n','f1:',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = [int(y) for y in Y]\n",
    "\n",
    "clf.fit(X, Y_true) # re-fit the data\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "precision = cross_val_score(clf, X, Y_true, cv = 10, scoring='precision')\n",
    "recall = cross_val_score(clf, X, Y_true, cv = 10, scoring='recall')\n",
    "f1 = cross_val_score(clf, X, Y_true, cv = 10, scoring='f1')\n",
    "\n",
    "print('precision:',precision.mean(),'\\n','recall:',recall.mean(),'\\n','f1:',f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
