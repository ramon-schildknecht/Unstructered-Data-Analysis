{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "1. Do text preprocessing (e.g., stopword removal, lemmatization, stemming, etc.)\n",
    "2. TF-IDF text representation\n",
    "3. Run LDA\n",
    "4. Identify the optimal number of topics\n",
    "5. Show top 10 words for each topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The dataset consists 2885 datasets information in 15 columns:\n",
    "\n",
    "- Title\n",
    "- Subtitle\n",
    "- Owner\n",
    "- Vote\n",
    "- Last update\n",
    "- Tags\n",
    "- Datatype\n",
    "- Size\n",
    "- License\n",
    "- Views\n",
    "- Downloads\n",
    "- Kernels\n",
    "- Topics\n",
    "- URL\n",
    "- Description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that the variables title, subtitle as well as description can be used for topic modeling. Therefore we start with title, go on with subtitle and finish with description to get the best possible topics out of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Load Packages & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# this code block was not suitable\n",
    "# read data\n",
    "# with open('dataset.csv','r') as fh:\n",
    "    lines = fh.readlines()\n",
    "# fh.close()\n",
    "# lines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pandas libraries with alias 'pd' \n",
    "import pandas as pd \n",
    "pd.options.display.max_rows = 10\n",
    "d = pd.read_csv(\"dataset.csv\") \n",
    "d\n",
    "d2 = d # backup for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d.shape\n",
    "d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Title\" Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove unnessesary characters - simple form of lemmatization\n",
    "d['title_preprocessed'] = d['Title'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "d['title_preprocessed'] = d['title_preprocessed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows\n",
    "d['title_preprocessed'].head() # success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check interim result \n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(d['title_preprocessed'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000,\n",
    "                      contour_width=3, contour_color='steelblue',\n",
    "                      height=400, width = 800)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "# stop. append(\"it\")\n",
    "print(stop)\n",
    "# title_no_stop_words.values.apply(lambda x: [item for item in x if item not in stop]) # first unsuccessful approach\n",
    "# d['title_preprocessed'] = pd.Series([word for word in d['title_preprocessed'] if word not in stop]) # second unsuccesful approach\n",
    "d['title_preprocessed'] = d['title_preprocessed'].str.lower().str.split() # old preprocessing step for following solution \n",
    "d['title_preprocessed'] = d['title_preprocessed'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check result\n",
    "print(list(d['Title'][15:16]))\n",
    "print(list(d['title_preprocessed'][15:16])) \n",
    "# success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "temp_list = []\n",
    "for words in list(d['title_preprocessed']):\n",
    "    temp_list.append(stemming.stem(\" \".join(words)))\n",
    "    \n",
    "    # print(words)\n",
    "    for word in words:\n",
    "        stemming.stem(word)\n",
    "        # print(word)\n",
    "        \n",
    "d['title_preprocessed'] = pd.Series(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "print(d['Title'][:10])\n",
    "print(\"\\n\"*2)\n",
    "print(d['title_preprocessed'][:10]) \n",
    "# success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_backup = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_features parameter passed in the TfidfVectorizer will pick out the top 50 features ordered by their TFIDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf transformation\n",
    "tfidf = tfidf_vectorizer.fit_transform(d['title_preprocessed'])\n",
    "tfidf.data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3, max_iter=5, # LDA, n_components = number of topics to be found\n",
    "                                learning_method='online', \n",
    "                                learning_offset=50., \n",
    "                                random_state=22) # set seed\n",
    "                                # doc_topic_prior = alpha = 0.01 per default\n",
    "                                # topic_word_prior = beta\n",
    "\n",
    "lda.fit(tfidf)\n",
    "\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every value above is connected to a word below (e. g. 8.70.... to '10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print out top 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [tfidf_feature_names[i] for i in topic.argsort()[:-20-1:-1]] # -10 = last 10 values & \n",
    "    print('Topic:',topic_idx,'--',top_words) # -1 because python starts at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic guessing:\n",
    "\n",
    "- Topic 0: Crime in India, e. g. https://www.kaggle.com/rajanand/crime-in-india\n",
    "- Topic 1: Predict Crypto Price Trends, e. g. https://cointelegraph.com/explained/how-to-predict-crypto-price-trends-explained\n",
    "- Topic 2: Wedding Announcements, e.g. https://www.nytimes.com/2019/05/26/fashion/weddings/this-weeks-wedding-announcements.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove unnessesary characters - simple form of lemmatization\n",
    "d['title_preprocessed'] = d['Title'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "d['title_preprocessed'] = d['title_preprocessed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows\n",
    "d['title_preprocessed'].head() # success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check interim result \n",
    "\n",
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(d['title_preprocessed'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000,\n",
    "                      contour_width=3, contour_color='steelblue',\n",
    "                      height=400, width = 800)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "# stop. append(\"it\")\n",
    "print(stop)\n",
    "# title_no_stop_words.values.apply(lambda x: [item for item in x if item not in stop]) # first unsuccessful approach\n",
    "# d['title_preprocessed'] = pd.Series([word for word in d['title_preprocessed'] if word not in stop]) # second unsuccesful approach\n",
    "d['title_preprocessed'] = d['title_preprocessed'].str.lower().str.split() # old preprocessing step for following solution \n",
    "d['title_preprocessed'] = d['title_preprocessed'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check result\n",
    "print(list(d['Title'][15:16]))\n",
    "print(list(d['title_preprocessed'][15:16])) \n",
    "# success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "temp_list = []\n",
    "for words in list(d['title_preprocessed']):\n",
    "    temp_list.append(stemming.stem(\" \".join(words)))\n",
    "    \n",
    "    # print(words)\n",
    "    for word in words:\n",
    "        stemming.stem(word)\n",
    "        # print(word)\n",
    "        \n",
    "d['title_preprocessed'] = pd.Series(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "print(d['Title'][:10])\n",
    "print(\"\\n\"*2)\n",
    "print(d['title_preprocessed'][:10]) \n",
    "# success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_backup = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max_features parameter passed in the TfidfVectorizer will pick out the top 50 features ordered by their TFIDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf transformation\n",
    "tfidf = tfidf_vectorizer.fit_transform(d['title_preprocessed'])\n",
    "tfidf.data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=3, max_iter=5, # LDA, n_components = number of topics to be found\n",
    "                                learning_method='online', \n",
    "                                learning_offset=50., \n",
    "                                random_state=22) # set seed\n",
    "                                # doc_topic_prior = alpha = 0.01 per default\n",
    "                                # topic_word_prior = beta\n",
    "\n",
    "lda.fit(tfidf)\n",
    "\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "every value above is connected to a word below (e. g. 8.70.... to '10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print out top 20 words\n",
    "One choose 20 instead of 10 words for better topic name finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [tfidf_feature_names[i] for i in topic.argsort()[:-20-1:-1]] # -10 = last 10 values & \n",
    "    print('Topic:',topic_idx,'--',top_words) # -1 because python starts at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic guessing:\n",
    "\n",
    "- Topic 0: Crime in India, e. g. https://www.kaggle.com/rajanand/crime-in-india\n",
    "- Topic 1: Predict Crypto Price Trends, e. g. https://cointelegraph.com/explained/how-to-predict-crypto-price-trends-explained\n",
    "- Topic 2: Wedding Announcements, e.g. https://www.nytimes.com/2019/05/26/fashion/weddings/this-weeks-wedding-announcements.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Description\" Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove unnessesary characters - simple form of lemmatization\n",
    "d['description_preprocessed'] = d['Description'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "d['description_preprocessed'] = d['description_preprocessed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows\n",
    "d['description_preprocessed'].head() # success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Title' handling approach is not working. We must use an own approach for transforming the 'Description' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check single item to get problematic patterns\n",
    "list(d['Description'][2:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problematic patterns are:\n",
    "- \\r\\n: meaning end of line in Windows\n",
    "- Hyperlinks could be problematic, too \n",
    "\n",
    "We ignore both patterns for the moment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new variable and lower all text\n",
    "d['description_preprocessed'] = d['Description'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d['description_preprocessed'][1])\n",
    "d['description_preprocessed'][1][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['description_preprocessed'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of rows with NAs in 'Description'\n",
    "d['description_preprocessed'].isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show rows with NAs in 'Description'\n",
    "nan_rows = d[d['description_preprocessed'].isnull()]\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows with NAs in variable 'Description'\n",
    "d = d.dropna(subset = ['description_preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "print(d['Description'][:10])\n",
    "print(\"\\n\"*2)\n",
    "print(d['description_preprocessed'][:10]) \n",
    "# success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(d['description_preprocessed'])\n",
    "# data = data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "def stemSentence(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    token_words\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)\n",
    "\n",
    "data_temp = []\n",
    "for sentence in data[:2875]:\n",
    "    # x = stemSentence(sentence)\n",
    "    data_temp.append(stemSentence(sentence))\n",
    "# sentence = data[1]\n",
    "# x=stemSentence(sentence)\n",
    "# print(data_temp)\n",
    "data = data_temp\n",
    "len(data_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000,\n",
    "                                   stop_words='english',\n",
    "                                   encoding = 'utf-8',\n",
    "                                   decode_error = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfidf transformation\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "tfidf.data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, max_iter=5, # LDA, n_components = number of topics to be found\n",
    "                                learning_method='online', # \n",
    "                                learning_offset=50., \n",
    "                                random_state=0) # set seed\n",
    "                                # doc_topic_prior = alpha = 0.01 per default\n",
    "                                # topic_word_prior = beta\n",
    "\n",
    "lda.fit(tfidf)\n",
    "\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print out top 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [tfidf_feature_names[i] for i in topic.argsort()[:-20-1:-1]] \n",
    "    print('Topic:',topic_idx,'--',top_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate based on perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pscores = []\n",
    "# n_topics = range(1, 31) #used for testing single outcomes. 5 steps are sufficient\n",
    "for n_topic in [2, 3, 4, 5, 10, 15, 20, 30]:\n",
    "    lda = LatentDirichletAllocation(n_components=n_topic, max_iter=5,random_state=7)\n",
    "\n",
    "    lda.fit(tfidf)\n",
    "\n",
    "    perplexity_score = lda.perplexity(tfidf)\n",
    "    print(perplexity_score)\n",
    "    pscores.append(perplexity_score)\n",
    "\n",
    "# pscores\n",
    "# perplexity score of 0 is best value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## plot the perplexity score with n_topics\n",
    "import matplotlib.pylab as plt\n",
    "plt.plot([2, 3, 4, 5, 10, 15, 20, 30],pscores,'r+--')\n",
    "plt.xlabel('# of topics')\n",
    "plt.ylabel('Perplexity score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: le245 = 10^245\n",
    "\n",
    "**Intepretation: up to 20 topics are reasonable, but no more. We use 5 topics as a heuristic and try to determine the topic names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_final = LatentDirichletAllocation(n_components=5, max_iter=5, \n",
    "                                learning_method='online', # \n",
    "                                learning_offset=50., \n",
    "                                random_state=0) # set seed\n",
    "\n",
    "lda_final.fit(tfidf)\n",
    "\n",
    "lda_final.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_feature_names[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print out top 20 words\n",
    "One choose 20 instead of 10 words for better topic name finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(lda_final.components_):\n",
    "    top_words = [tfidf_feature_names[i] for i in topic.argsort()[:-20-1:-1]] \n",
    "    print('Topic:',topic_idx,'--',top_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of results\n",
    "\n",
    "Results could show be the following topics:\n",
    "- Topic 1 (index 0): IMDB Movie Reviews, e. g. article [IMDB Movie Reviews Dataset](https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset)\n",
    "- Topic 2 (index 1): Text Processing with Python, e. g. article [Machine Learning — Text Processing – Towards Data Science](https://towardsdatascience.com/machine-learning-text-processing-1d5a2d638958)\n",
    "- Topic 3 (index 2): Pokemon behaviour, e. g. article [Pokemon Moves](https://www.poke-verse.com/pokemon-moves/)\n",
    "- Topic 4 (index 3): Document Clustering, e. g. article [Self-Tuned Descriptive Document Clustering](http://pcwww.liv.ac.uk/~goulerma/publications/descr-clust_preprint_full.pdf)\n",
    "- Topic 5 (index 4): Free Data Sets for Data Science Projects, e. g. article [Free Data Sets for Data Science Projects – Dataquest](https://www.dataquest.io/blog/free-datasets-for-projects/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could perform additional or alternative text preprocessing transformations such as\n",
    "    \n",
    "- lemmatization (as alternative to stemming)\n",
    "- remove URLs\n",
    "- remove punctuation\n",
    "- remove pattern '\\r\\n', e. g. using regex\n",
    "\n",
    "to check improvement on LDA outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit one python Jupyter notebook file with the filename following: YourFirstName_YourLastName.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
